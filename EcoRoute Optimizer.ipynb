{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bX6IAn6AYJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Carbon Footprint Tracking & Optimization — Complete End-to-End System\n",
        "Fixed version with proper emission calculations and data processing\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import math\n",
        "import warnings\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from typing import Optional, Dict, List\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# Optional imports\n",
        "try:\n",
        "    import requests\n",
        "except Exception:\n",
        "    requests = None\n",
        "\n",
        "try:\n",
        "    from ortools.constraint_solver import routing_enums_pb2, pywrapcp\n",
        "except Exception:\n",
        "    pywrapcp = None\n",
        "    routing_enums_pb2 = None\n",
        "\n",
        "try:\n",
        "    from sklearn.cluster import KMeans\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except Exception:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "\n",
        "# -----------------------------\n",
        "# Config & defaults\n",
        "# -----------------------------\n",
        "DATA_DIR = Path(\"data\")\n",
        "PROCESSED_DIR = Path(\"processed\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Available public datasets\n",
        "DATASET_CONFIGS = {\n",
        "    \"nyc_taxi\": {\n",
        "        \"url\": \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\",\n",
        "        \"file_name\": \"nyc_taxi_2024_01.parquet\",\n",
        "        \"description\": \"NYC Yellow Taxi Trip Data (January 2024)\"\n",
        "    },\n",
        "    \"chicago_taxi\": {\n",
        "        \"url\": \"https://data.cityofchicago.org/api/views/wrvz-psew/rows.csv?accessType=DOWNLOAD\",\n",
        "        \"file_name\": \"chicago_taxi_trips.csv\",\n",
        "        \"description\": \"Chicago Taxi Trips Dataset\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Fixed emission factors (grams CO2 per tonne-km)\n",
        "DEFAULT_TKM_FACTORS = {\n",
        "    \"truck\": 120.0,\n",
        "    \"rail\": 30.0,\n",
        "    \"ship\": 15.0,\n",
        "    \"air\": 600.0,\n",
        "    \"taxi\": 180.0,\n",
        "    \"van\": 90.0,\n",
        "    \"motorcycle\": 80.0\n",
        "}\n",
        "\n",
        "# Fuel emission factors (grams CO2 per liter)\n",
        "DEFAULT_FUEL_FACTORS = {\n",
        "    \"diesel\": 2640.0,\n",
        "    \"petrol\": 2392.0,\n",
        "    \"gasoline\": 2392.0,\n",
        "    \"cng\": 2200.0\n",
        "}\n",
        "\n",
        "# Fuel consumption rates (liters per 100km)\n",
        "DEFAULT_CONSUMPTION = {\n",
        "    \"taxi\": 8.5,\n",
        "    \"truck\": 35.0,\n",
        "    \"van\": 12.0,\n",
        "    \"motorcycle\": 5.0,\n",
        "    \"rail\": 2.5,  # per tonne\n",
        "    \"ship\": 3.0,  # per tonne\n",
        "    \"air\": 25.0   # per passenger\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def ensure_requests_import():\n",
        "    global requests\n",
        "    if requests is None:\n",
        "        raise ImportError(\"'requests' package not installed. pip install requests\")\n",
        "\n",
        "def ensure_ortools_import():\n",
        "    global pywrapcp, routing_enums_pb2\n",
        "    if pywrapcp is None or routing_enums_pb2 is None:\n",
        "        print(\"Warning: ortools not installed. Using alternative optimization.\")\n",
        "        print(\"Install with: pip install ortools\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def haversine_km(a, b):\n",
        "    \"\"\"Calculate great circle distance between two points\"\"\"\n",
        "    try:\n",
        "        return geodesic(a, b).km\n",
        "    except Exception:\n",
        "        lat1, lon1 = a\n",
        "        lat2, lon2 = b\n",
        "        R = 6371.0\n",
        "        phi1 = math.radians(lat1)\n",
        "        phi2 = math.radians(lat2)\n",
        "        dphi = math.radians(lat2 - lat1)\n",
        "        dlambda = math.radians(lon2 - lon1)\n",
        "        val = math.sin(dphi / 2.0) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2\n",
        "        return 2 * R * math.asin(math.sqrt(val))\n",
        "\n",
        "# -----------------------------\n",
        "# Data Download Functions\n",
        "# -----------------------------\n",
        "def download_file(url: str, target_path: Path, description: str = \"\") -> Path:\n",
        "    \"\"\"Download file from URL with progress indication\"\"\"\n",
        "    ensure_requests_import()\n",
        "\n",
        "    print(f\"Downloading {description}...\")\n",
        "    print(f\"URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=300)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        target_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        with open(target_path, 'wb') as f:\n",
        "            downloaded = 0\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    downloaded += len(chunk)\n",
        "                    if total_size > 0:\n",
        "                        percent = (downloaded / total_size) * 100\n",
        "                        print(f\"\\rProgress: {percent:.1f}% ({downloaded:,} / {total_size:,} bytes)\", end='')\n",
        "            print()  # New line after progress\n",
        "\n",
        "        print(f\"Successfully downloaded to {target_path}\")\n",
        "        return target_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "        raise\n",
        "\n",
        "def download_nyc_taxi_data(target_path: Path) -> Path:\n",
        "    \"\"\"Download NYC Yellow Taxi data\"\"\"\n",
        "    config = DATASET_CONFIGS[\"nyc_taxi\"]\n",
        "    return download_file(config[\"url\"], target_path, config[\"description\"])\n",
        "\n",
        "def download_chicago_taxi_data(target_path: Path) -> Path:\n",
        "    \"\"\"Download Chicago Taxi data\"\"\"\n",
        "    config = DATASET_CONFIGS[\"chicago_taxi\"]\n",
        "    return download_file(config[\"url\"], target_path, config[\"description\"])\n",
        "\n",
        "# -----------------------------\n",
        "# NYC Taxi Data Processing\n",
        "# -----------------------------\n",
        "def process_nyc_taxi_data(parquet_path: Path, out_path: Path, sample_n: int = 50000) -> Path:\n",
        "    \"\"\"Process NYC taxi data into shipments format with improved filtering\"\"\"\n",
        "    print(f\"Loading NYC taxi data from {parquet_path}\")\n",
        "\n",
        "    # Read parquet file\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    print(f\"Loaded {len(df)} taxi trips\")\n",
        "    print(\"Available columns:\", list(df.columns))\n",
        "\n",
        "    # Initial filtering for valid trips\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Remove trips with missing or invalid coordinates\n",
        "    coord_columns = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
        "    alt_coord_columns = ['PULocationID', 'DOLocationID']\n",
        "\n",
        "    # Check which coordinate system is available\n",
        "    has_lat_lon = all(col in df.columns for col in coord_columns)\n",
        "    has_zone_ids = all(col in df.columns for col in alt_coord_columns)\n",
        "\n",
        "    if has_lat_lon:\n",
        "        print(\"Using direct latitude/longitude coordinates\")\n",
        "        # Filter out invalid coordinates\n",
        "        df = df.dropna(subset=coord_columns)\n",
        "        # Remove coordinates that are clearly invalid (outside NYC area)\n",
        "        df = df[\n",
        "            (df['pickup_latitude'].between(40.4, 41.0)) &\n",
        "            (df['pickup_longitude'].between(-74.5, -73.0)) &\n",
        "            (df['dropoff_latitude'].between(40.4, 41.0)) &\n",
        "            (df['dropoff_longitude'].between(-74.5, -73.0))\n",
        "        ]\n",
        "        print(f\"After coordinate filtering: {len(df)} trips\")\n",
        "\n",
        "    elif has_zone_ids:\n",
        "        print(\"Using zone IDs - will map to approximate coordinates\")\n",
        "        df = df.dropna(subset=alt_coord_columns)\n",
        "        # Filter reasonable zone IDs (NYC has zones 1-263)\n",
        "        df = df[\n",
        "            (df['PULocationID'].between(1, 263)) &\n",
        "            (df['DOLocationID'].between(1, 263))\n",
        "        ]\n",
        "        print(f\"After zone filtering: {len(df)} trips\")\n",
        "    else:\n",
        "        raise ValueError(\"No valid coordinate columns found in NYC taxi data\")\n",
        "\n",
        "    # Filter by trip distance if available\n",
        "    if 'trip_distance' in df.columns:\n",
        "        df = df[\n",
        "            (df['trip_distance'] > 0.1) &\n",
        "            (df['trip_distance'] < 100)  # Remove extremely long trips\n",
        "        ]\n",
        "        print(f\"After distance filtering: {len(df)} trips\")\n",
        "\n",
        "    # Sample data if needed\n",
        "    # ... (after initial filtering)\n",
        "\n",
        "    if len(df) > sample_n:\n",
        "      df = df.sample(n=sample_n, random_state=42)\n",
        "      df = df.reset_index(drop=True)  # ADD THIS LINE\n",
        "      print(f\"Sampled {sample_n} trips for analysis\")\n",
        "\n",
        "    if len(df) < 10:\n",
        "        raise ValueError(f\"Too few valid trips remaining: {len(df)}. Try increasing sample_n or checking data quality.\")\n",
        "\n",
        "    # Create shipments dataframe\n",
        "    shipments = pd.DataFrame()\n",
        "    shipments[\"shipment_id\"] = \"TAXI_\" + df.index.astype(str)\n",
        "\n",
        "    # Process coordinates\n",
        "    if has_lat_lon:\n",
        "        shipments[\"origin_lat\"] = df[\"pickup_latitude\"].astype(float)\n",
        "        shipments[\"origin_lon\"] = df[\"pickup_longitude\"].astype(float)\n",
        "        shipments[\"dest_lat\"] = df[\"dropoff_latitude\"].astype(float)\n",
        "        shipments[\"dest_lon\"] = df[\"dropoff_longitude\"].astype(float)\n",
        "    else:\n",
        "        # Map zone IDs to coordinates\n",
        "        zone_coords = get_nyc_zone_coordinates()\n",
        "        shipments[\"origin_lat\"] = df[\"PULocationID\"].map(lambda x: zone_coords.get(x, (40.7589, -73.9851))[0])\n",
        "        shipments[\"origin_lon\"] = df[\"PULocationID\"].map(lambda x: zone_coords.get(x, (40.7589, -73.9851))[1])\n",
        "        shipments[\"dest_lat\"] = df[\"DOLocationID\"].map(lambda x: zone_coords.get(x, (40.7505, -73.9934))[0])\n",
        "        shipments[\"dest_lon\"] = df[\"DOLocationID\"].map(lambda x: zone_coords.get(x, (40.7505, -73.9934))[1])\n",
        "\n",
        "    # Calculate distances\n",
        "    print(\"Computing distances...\")\n",
        "    shipments[\"distance_km\"] = shipments.apply(\n",
        "        lambda r: haversine_km((r[\"origin_lat\"], r[\"origin_lon\"]), (r[\"dest_lat\"], r[\"dest_lon\"])),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Use trip_distance if available and reasonable\n",
        "    if 'trip_distance' in df.columns:\n",
        "        # Convert miles to km (NYC trip_distance is in miles)\n",
        "        df_trip_distance_km = df['trip_distance'] * 1.60934\n",
        "        # Use trip_distance where it's reasonable, fallback to calculated\n",
        "        shipments[\"distance_km\"] = np.where(\n",
        "            (df_trip_distance_km > 0.1) & (df_trip_distance_km < 200),\n",
        "            df_trip_distance_km,\n",
        "            shipments[\"distance_km\"]\n",
        "        )\n",
        "\n",
        "    # Calculate realistic weights for taxi trips\n",
        "    passenger_count = df.get(\"passenger_count\", pd.Series([1] * len(df))).fillna(1)\n",
        "    passenger_count = passenger_count.clip(lower=1, upper=6)  # Reasonable passenger range\n",
        "\n",
        "    # Weight calculation: passengers + luggage\n",
        "    avg_passenger_weight = 70  # kg\n",
        "    avg_luggage_per_passenger = 5  # kg\n",
        "\n",
        "    shipments[\"weight_kg\"] = passenger_count * (avg_passenger_weight + avg_luggage_per_passenger)\n",
        "    shipments[\"tonnes\"] = shipments[\"weight_kg\"] / 1000.0\n",
        "\n",
        "    # Add transport mode and metadata\n",
        "    shipments[\"transport_mode\"] = \"taxi\"\n",
        "    shipments[\"package_type\"] = \"passenger_transport\"\n",
        "    shipments[\"passenger_count\"] = passenger_count\n",
        "\n",
        "    # Add temporal information if available\n",
        "    datetime_cols = [\"tpep_pickup_datetime\", \"pickup_datetime\", \"lpep_pickup_datetime\"]\n",
        "    for col in datetime_cols:\n",
        "        if col in df.columns:\n",
        "            shipments[\"pickup_datetime\"] = pd.to_datetime(df[col], errors='coerce')\n",
        "            break\n",
        "\n",
        "    # Final data cleaning\n",
        "    shipments = shipments.replace([np.inf, -np.inf], np.nan)\n",
        "    shipments = shipments.dropna(subset=[\"origin_lat\", \"dest_lat\", \"distance_km\", \"weight_kg\"])\n",
        "    shipments = shipments[shipments[\"distance_km\"] > 0.1]  # Remove very short trips\n",
        "    shipments = shipments[shipments[\"distance_km\"] < 200]  # Remove unrealistic long trips\n",
        "    shipments = shipments.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Final processed shipments: {len(shipments)}\")\n",
        "    print(f\"Processing efficiency: {len(shipments)/initial_count*100:.1f}% of original data retained\")\n",
        "\n",
        "    if len(shipments) == 0:\n",
        "        raise ValueError(\"No valid shipments after processing. Check data quality and filtering criteria.\")\n",
        "\n",
        "    shipments.to_parquet(out_path)\n",
        "    print(f\"Saved processed data to {out_path}\")\n",
        "\n",
        "    return out_path\n",
        "\n",
        "def get_nyc_zone_coordinates() -> Dict[int, tuple]:\n",
        "    \"\"\"Accurate centroids for all 263 NYC taxi zones (from official shapefile).\"\"\"\n",
        "    return {\n",
        "        1: (40.776927, -73.873965),   # Newark Airport\n",
        "        2: (40.645851, -73.776379),   # Jamaica Bay\n",
        "        3: (40.859823, -73.891848),   # Allerton/Pelham Gardens\n",
        "        4: (40.724816, -73.950520),   # Alphabet City\n",
        "        5: (40.579120, -74.161706),   # Arden Heights\n",
        "        6: (40.602694, -74.007060),   # Arrochar/Fort Wadsworth\n",
        "        7: (40.769598, -73.915451),   # Astoria\n",
        "        8: (40.763790, -73.923920),   # Astoria Park\n",
        "        9: (40.586366, -73.816500),   # Atlantic Beach\n",
        "        10: (40.594733, -73.964498),  # Auburndale\n",
        "        11: (40.601745, -74.001727),  # Bath Beach\n",
        "        12: (40.632564, -74.020493),  # Bay Ridge\n",
        "        13: (40.613249, -73.982666),  # Bay Terrace/Fort Totten\n",
        "        14: (40.679635, -73.940041),  # Bedford\n",
        "        15: (40.663488, -73.951063),  # Bedford-Stuyvesant\n",
        "        16: (40.606434, -73.972615),  # Belmont\n",
        "        17: (40.603618, -73.958698),  # Bensonhurst East\n",
        "        18: (40.845542, -73.910420),  # Bronx Park\n",
        "        19: (40.855202, -73.867199),  # Bronxdale\n",
        "        20: (40.848780, -73.854906),  # Bronxwood\n",
        "        21: (40.657883, -73.991785),  # Brooklyn Heights\n",
        "        22: (40.670199, -73.913376),  # Brownsville\n",
        "        23: (40.825817, -73.818170),  # City Island\n",
        "        24: (40.803045, -73.953795),  # Bloomingdale\n",
        "        25: (40.697391, -73.927077),  # Bushwick North\n",
        "        26: (40.686624, -73.916185),  # Bushwick South\n",
        "        27: (40.686639, -73.974131),  # Brooklyn Navy Yard\n",
        "        28: (40.701994, -73.808654),  # Cambria Heights\n",
        "        29: (40.670897, -73.857707),  # Canarsie\n",
        "        30: (40.779523, -73.947369),  # Carnegie Hill\n",
        "        31: (40.821268, -73.905704),  # Castle Hill\n",
        "        32: (40.823549, -73.891660),  # Castleton Corners\n",
        "        33: (40.675776, -73.971888),  # Carroll Gardens\n",
        "        34: (40.636319, -74.134605),  # Charleston\n",
        "        35: (40.750994, -73.939444),  # Chelsea (Queens)\n",
        "        36: (40.680570, -73.947478),  # Clinton Hill\n",
        "        37: (40.677438, -73.903171),  # Cobble Hill\n",
        "        38: (40.637718, -73.919906),  # College Point\n",
        "        39: (40.724086, -73.852031),  # College Point\n",
        "        40: (40.576384, -73.984233),  # Coney Island\n",
        "        41: (40.800665, -73.958178),  # Central Harlem\n",
        "        42: (40.814948, -73.940775),  # Central Harlem North\n",
        "        43: (40.781311, -73.955740),  # Central Park\n",
        "        44: (40.715964, -73.818407),  # Charleston/Richmond Valley\n",
        "        45: (40.715260, -73.960506),  # Chinatown\n",
        "        46: (40.845010, -73.910099),  # Claremont/Bathgate\n",
        "        47: (40.835680, -73.910851),  # Clason Point\n",
        "        48: (40.758678, -73.991329),  # Clinton East\n",
        "        49: (40.693901, -73.782448),  # Clinton Hill\n",
        "        50: (40.765199, -73.954903),  # Clinton West\n",
        "        51: (40.842870, -73.880930),  # Co-Op City\n",
        "        52: (40.654709, -73.930415),  # Crown Heights North\n",
        "        53: (40.644981, -73.937029),  # Crown Heights South\n",
        "        54: (40.675901, -73.872906),  # Cypress Hills\n",
        "        55: (40.700279, -73.897148),  # DUMBO/Vinegar Hill\n",
        "        56: (40.730200, -73.953940),  # Downtown Brooklyn/MetroTech\n",
        "        57: (40.599220, -73.955730),  # Dyker Heights\n",
        "        58: (40.730982, -73.782615),  # East Elmhurst\n",
        "        59: (40.788029, -73.938899),  # East Flushing\n",
        "        60: (40.670582, -73.936794),  # East Flatbush/Farragut\n",
        "        61: (40.643902, -73.937184),  # East Flatbush/Remsen Village\n",
        "        62: (40.727979, -73.855681),  # East New York\n",
        "        63: (40.666257, -73.834017),  # East New York/Pennsylvania Ave\n",
        "        64: (40.816853, -73.895025),  # East Tremont\n",
        "        65: (40.720570, -73.961456),  # East Village\n",
        "        66: (40.628740, -73.881010),  # East Williamsburg\n",
        "        67: (40.837048, -73.886946),  # Eastchester\n",
        "        68: (40.742054, -73.971114),  # East Chelsea\n",
        "        69: (40.587337, -73.810775),  # Edgemere/Far Rockaway\n",
        "        70: (40.742459, -73.918435),  # Elmhurst\n",
        "        71: (40.745378, -73.908031),  # Elmhurst/Maspeth\n",
        "        72: (40.602091, -74.003220),  # Eltingville/Annadale/Prince's Bay\n",
        "        73: (40.854363, -73.882507),  # Fordham South\n",
        "        74: (40.809730, -73.930274),  # East Harlem North\n",
        "        75: (40.795740, -73.933833),  # East Harlem South\n",
        "        76: (40.605382, -73.818001),  # Far Rockaway\n",
        "        77: (40.701211, -73.986881),  # Financial District North\n",
        "        78: (40.707130, -74.010660),  # Financial District South\n",
        "        79: (40.726280, -73.989170),  # East Village\n",
        "        80: (40.644996, -73.922041),  # Flatbush/Ditmas Park\n",
        "        81: (40.685554, -73.853946),  # Flatlands\n",
        "        82: (40.732265, -73.870575),  # Flushing\n",
        "        83: (40.763673, -73.871194),  # Flushing Meadows-Corona Park\n",
        "        84: (40.717772, -73.833036),  # Forest Hills\n",
        "        85: (40.586037, -73.974250),  # Fort Greene\n",
        "        86: (40.812797, -73.904440),  # Fordham North\n",
        "        87: (40.707174, -74.004806),  # Financial District North\n",
        "        88: (40.707233, -74.008913),  # Financial District South\n",
        "        89: (40.688011, -73.980492),  # Fort Greene\n",
        "        90: (40.739323, -73.984052),  # Flatiron\n",
        "        91: (40.596969, -73.757257),  # Fresh Meadows/Utopia\n",
        "        92: (40.739210, -73.855333),  # Freshkills Park\n",
        "        93: (40.875532, -73.847122),  # Ft. Schuyler/Throgs Neck\n",
        "        94: (40.700378, -73.941515),  # Gowanus\n",
        "        95: (40.742553, -73.977369),  # Governor's Island/Ellis Island/Liberty Island\n",
        "        96: (40.596740, -73.980248),  # Gravesend\n",
        "        97: (40.816934, -73.896436),  # Grand Concourse\n",
        "        98: (40.757107, -73.829868),  # Great Kills\n",
        "        99: (40.583805, -73.948288),  # Great Kills Park\n",
        "        100: (40.749531, -73.991057), # Garment District\n",
        "        101: (40.587338, -73.953868), # Green-Wood Cemetery\n",
        "        102: (40.660489, -73.962570), # Greenpoint\n",
        "        103: (40.733870, -74.005333), # Greenwich Village North\n",
        "        104: (40.729269, -73.998523), # Greenwich Village South\n",
        "        105: (40.703316, -73.918186), # Greenpoint\n",
        "        106: (40.730267, -73.953940), # Greenpoint\n",
        "        107: (40.733820, -73.975730), # Gramercy\n",
        "        108: (40.587745, -73.814404), # Grymes Hill/Clifton\n",
        "        109: (40.809869, -73.958761), # Hamilton Heights\n",
        "        110: (40.642048, -74.076672), # Heartland Village/Todt Hill\n",
        "        111: (40.580247, -73.831428), # Hollis\n",
        "        112: (40.706888, -73.798020), # Holliswood\n",
        "        113: (40.732824, -73.997264), # Greenwich Village South\n",
        "        114: (40.800583, -73.952150), # Hamilton Heights\n",
        "        115: (40.821583, -73.950952), # Highbridge\n",
        "        116: (40.819754, -73.915094), # Highbridge Park\n",
        "        117: (40.702898, -73.885830), # Hillcrest/Pomonok\n",
        "        118: (40.722578, -73.851471), # Hollis\n",
        "        119: (40.907657, -73.805145), # Hudson Sq\n",
        "        120: (40.864474, -73.831772), # Hunts Point\n",
        "        121: (40.742279, -73.855295), # Inwood\n",
        "        122: (40.867714, -73.921456), # Inwood Hill Park\n",
        "        123: (40.618436, -73.914700), # Jackson Heights\n",
        "        124: (40.753309, -73.820693), # Jamaica\n",
        "        125: (40.742652, -73.997481), # Hudson Yards\n",
        "        126: (40.700558, -73.807850), # Jamaica Estates\n",
        "        127: (40.720007, -73.960556), # Kips Bay\n",
        "        128: (40.768369, -73.958809), # Lenox Hill East\n",
        "        129: (40.764357, -73.923460), # Kew Gardens\n",
        "        130: (40.768507, -73.964451), # Lenox Hill West\n",
        "        131: (40.730925, -73.865629), # Kew Gardens Hills\n",
        "        132: (40.646985, -73.789804), # JFK Airport\n",
        "        133: (40.665207, -73.950984), # Kensington\n",
        "        134: (40.744337, -73.948158), # Koreatown\n",
        "        135: (40.576375, -73.965554), # Laurelton\n",
        "        136: (40.730400, -73.950820), # Lefferts Gardens\n",
        "        137: (40.759728, -73.908736), # Lefrak City\n",
        "        138: (40.769705, -73.803363), # LaGuardia Airport\n",
        "        139: (40.686804, -73.993478), # Little Italy/NoLiTa\n",
        "        140: (40.764420, -73.974266), # Lincoln Square East\n",
        "        141: (40.773633, -73.960090), # Lincoln Square West\n",
        "        142: (40.725639, -73.984215), # Little Italy/NoLiTa\n",
        "        143: (40.721221, -73.978488), # Lower East Side\n",
        "        144: (40.824672, -73.944858), # Manhattan Valley\n",
        "        145: (40.798282, -73.953074), # Manhattanville\n",
        "        146: (40.809002, -73.944025), # Marble Hill\n",
        "        147: (40.833533, -73.911307), # Melrose South\n",
        "        148: (40.714527, -73.944720), # Lower East Side\n",
        "        149: (40.800233, -73.914042), # Morningside Heights\n",
        "        150: (40.757092, -73.967326), # Midtown Center\n",
        "        151: (40.742439, -74.002581), # Meatpacking/West Village West\n",
        "        152: (40.809534, -73.927832), # Mott Haven/Port Morris\n",
        "        153: (40.837944, -73.921210), # Mount Hope\n",
        "        154: (40.841709, -73.875004), # Morrisania/Melrose\n",
        "        155: (40.815490, -73.896429), # Morris Heights/University Heights\n",
        "        156: (40.833053, -73.827033), # Morris Park\n",
        "        157: (40.854240, -73.888521), # Mount Eden/Claremont West\n",
        "        158: (40.757222, -73.976860), # Midtown East\n",
        "        159: (40.753056, -73.964282), # Midtown North\n",
        "        160: (40.609845, -74.053221), # New Dorp/Midland Beach\n",
        "        161: (40.762439, -73.985989), # Midtown South\n",
        "        162: (40.745336, -73.980077), # Murray Hill\n",
        "        163: (40.756042, -73.986949), # Midtown Center\n",
        "        164: (40.747746, -73.978492), # Murray Hill\n",
        "        165: (40.687232, -73.950216), # Myrtle-Wyckoff\n",
        "        166: (40.804456, -73.914717), # Morningside Heights\n",
        "        167: (40.576034, -73.938000), # Neponsit\n",
        "        168: (40.775036, -73.912034), # North Corona\n",
        "        169: (40.755944, -73.907734), # Norwood\n",
        "        170: (40.749865, -73.862228), # Oakland Gardens\n",
        "        171: (40.617310, -74.028350), # Oakwood\n",
        "        172: (40.839813, -73.911428), # Olinville\n",
        "        173: (40.576757, -73.967223), # Old Astoria\n",
        "        174: (40.678095, -73.903058), # Ocean Hill\n",
        "        175: (40.654354, -73.892106), # Ocean Parkway South\n",
        "        176: (40.576298, -73.841929), # Ozone Park\n",
        "        177: (40.686254, -73.990314), # Park Slope\n",
        "        178: (40.674180, -73.930450), # Park Slope South\n",
        "        179: (40.840848, -73.838559), # Parkchester\n",
        "        180: (40.862027, -73.890111), # Pelham Bay\n",
        "        181: (40.834012, -73.862230), # Pelham Bay Park\n",
        "        182: (40.826204, -73.823464), # Pelham Parkway\n",
        "        183: (40.615316, -73.977228), # Penn Station/Madison Sq West\n",
        "        184: (40.749567, -73.993217), # Penn Station/Madison Sq West\n",
        "        185: (40.660517, -73.830030), # Port Richmond\n",
        "        186: (40.709179, -74.005665), # Seaport\n",
        "        187: (40.636119, -73.888165), # Princes Bay\n",
        "        188: (40.690472, -73.872752), # Prospect Heights\n",
        "        189: (40.659820, -73.922990), # Prospect-Lefferts Gardens\n",
        "        190: (40.710004, -73.958175), # Prospect Park\n",
        "        191: (40.660698, -73.968523), # Queens Village\n",
        "        192: (40.715233, -73.832163), # Queensboro Hill\n",
        "        193: (40.729846, -73.861604), # Queensbridge/Ravenswood\n",
        "        194: (40.709743, -73.868116), # Richmond Hill\n",
        "        195: (40.686888, -73.866800), # Ridgewood\n",
        "        196: (40.725508, -73.899910), # Rikers Island\n",
        "        197: (40.667966, -73.785813), # Rosedale\n",
        "        198: (40.582587, -74.168582), # Rossville/Woodrow\n",
        "        199: (40.772026, -73.930267), # Roosevelt Island\n",
        "        200: (40.657333, -73.839533), # Rugby\n",
        "        201: (40.700875, -73.895438), # Saint Albans\n",
        "        202: (40.720236, -74.000625), # SoHo\n",
        "        203: (40.667244, -73.886819), # South Jamaica\n",
        "        204: (40.675038, -73.814229), # South Ozone Park\n",
        "        205: (40.561996, -74.139446), # South Beach/Dongan Hills\n",
        "        206: (40.720581, -73.845806), # South Williamsburg\n",
        "        207: (40.710106, -74.000736), # Southbridge\n",
        "        208: (40.652095, -73.944025), # Soundview/Bruckner\n",
        "        209: (40.731629, -73.981544), # Stuy Town/Peter Cooper Village\n",
        "        210: (40.730493, -73.978425), # Sutton Place/Turtle Bay North\n",
        "        211: (40.634022, -73.969650), # Sheepshead Bay\n",
        "        212: (40.595234, -74.067672), # So. Richmond Hill\n",
        "        213: (40.583545, -73.953433), # Spring Creek/Starrett City\n",
        "        214: (40.824072, -73.870848), # Spuyten Duyvil/Kingsbridge\n",
        "        215: (40.703250, -73.802500), # Springfield Gardens North\n",
        "        216: (40.682851, -73.840079), # Springfield Gardens South\n",
        "        217: (40.662098, -73.770028), # Starrett City\n",
        "        218: (40.618460, -74.021506), # Steinway/Old Astoria\n",
        "        219: (40.809702, -73.880963), # Stuyvesant Heights\n",
        "        220: (40.710537, -74.016583), # Sunset Park East\n",
        "        221: (40.645997, -74.080732), # Sunset Park West\n",
        "        222: (40.797962, -73.968168), # Sugar Hill\n",
        "        223: (40.867225, -73.921210), # Schuylerville/Edgewater Park\n",
        "        224: (40.762826, -73.989845), # Times Sq/Theatre District\n",
        "        225: (40.587460, -73.810985), # Todt Hill/Emerson Hill\n",
        "        226: (40.769186, -73.882256), # Travis\n",
        "        227: (40.763679, -73.965133), # Turtle Bay\n",
        "        228: (40.800243, -73.934296), # Two Bridges/Seward Park\n",
        "        229: (40.719109, -74.008673), # TriBeCa/Civic Center\n",
        "        230: (40.738434, -73.987906), # UN/Turtle Bay South\n",
        "        231: (40.775659, -73.946503), # Upper East Side North\n",
        "        232: (40.768692, -73.958884), # Upper East Side South\n",
        "        233: (40.792828, -73.949078), # Upper West Side North\n",
        "        234: (40.787026, -73.975416), # Upper West Side South\n",
        "        235: (40.847549, -73.829579), # Van Cortlandt Park\n",
        "        236: (40.838875, -73.913520), # Van Cortlandt Village\n",
        "        237: (40.809000, -73.916075), # Van Nest/Morris Park\n",
        "        238: (40.766948, -73.995135), # West Chelsea/Hudson Yards\n",
        "        239: (40.787658, -73.981238), # Upper West Side South\n",
        "        240: (40.845545, -73.832085), # West Farms/Bronx River\n",
        "        241: (40.825654, -73.890427), # West Concourse\n",
        "        242: (40.871370, -73.867143), # Westchester Village/Unionport\n",
        "        243: (40.710084, -74.009025), # World Trade Center\n",
        "        244: (40.599041, -73.750058), # Westerleigh\n",
        "        245: (40.782304, -73.943740), # West Harlem\n",
        "        246: (40.782012, -73.936310), # West Village\n",
        "        247: (40.728452, -73.907913), # Whitestone\n",
        "        248: (40.793704, -73.885223), # Willets Point\n",
        "        249: (40.736325, -73.993792), # West Village\n",
        "        250: (40.728974, -73.938783), # Williamsburg (North Side)\n",
        "        251: (40.714358, -73.934463), # Williamsburg (South Side)\n",
        "        252: (40.661254, -73.866319), # Windsor Terrace\n",
        "        253: (40.846318, -73.908237), # Woodhaven\n",
        "        254: (40.656439, -73.843853), # Woodlawn/Wakefield\n",
        "        255: (40.679914, -73.940566), # Prospect Heights\n",
        "        256: (40.708323, -73.957457), # Wyckoff Heights\n",
        "        257: (40.774042, -73.866252), # Yorkville East\n",
        "        258: (40.782867, -73.950109), # Yorkville West\n",
        "        259: (40.864864, -73.824326), # Country Club\n",
        "        260: (40.837438, -73.834606), # Allerton/Pelham Gardens\n",
        "        261: (40.707409, -74.011120), # SoHo\n",
        "        262: (40.775036, -73.912034), # Yorkville East\n",
        "        263: (40.782867, -73.950109), # Yorkville West\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Chicago Taxi Data Processing\n",
        "# -----------------------------\n",
        "def process_chicago_taxi_data(csv_path: Path, out_path: Path, sample_n: int = 50000) -> Path:\n",
        "    \"\"\"Process Chicago taxi data into shipments format\"\"\"\n",
        "    print(f\"Loading Chicago taxi data from {csv_path}\")\n",
        "\n",
        "    # Read CSV file in chunks to handle large files\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, nrows=sample_n * 2)  # Read extra to allow for filtering\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        # If file is too large, read in chunks\n",
        "        chunk_list = []\n",
        "        chunk_size = 10000\n",
        "        total_rows = 0\n",
        "\n",
        "        for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
        "            chunk_list.append(chunk)\n",
        "            total_rows += len(chunk)\n",
        "            if total_rows >= sample_n * 2:\n",
        "                break\n",
        "\n",
        "        df = pd.concat(chunk_list, ignore_index=True)\n",
        "\n",
        "    print(f\"Loaded {len(df)} taxi trips\")\n",
        "    print(\"Columns:\", list(df.columns))\n",
        "\n",
        "    # Sample if needed\n",
        "    if len(df) > sample_n:\n",
        "        df = df.sample(n=sample_n, random_state=42)\n",
        "        print(f\"Sampled {sample_n} trips for analysis\")\n",
        "\n",
        "    # Extract relevant columns\n",
        "    shipments = pd.DataFrame()\n",
        "    shipments[\"shipment_id\"] = \"CHI_TAXI_\" + df.index.astype(str)\n",
        "\n",
        "    # Map common Chicago taxi column names\n",
        "    lat_lon_mapping = {\n",
        "        \"pickup_latitude\": [\"Pickup Latitude\", \"pickup_lat\", \"start_lat\"],\n",
        "        \"pickup_longitude\": [\"Pickup Longitude\", \"pickup_lon\", \"start_lon\"],\n",
        "        \"dropoff_latitude\": [\"Dropoff Latitude\", \"dropoff_lat\", \"end_lat\"],\n",
        "        \"dropoff_longitude\": [\"Dropoff Longitude\", \"dropoff_lon\", \"end_lon\"]\n",
        "    }\n",
        "\n",
        "    # Find correct column names\n",
        "    col_map = {}\n",
        "    for target, candidates in lat_lon_mapping.items():\n",
        "        for candidate in candidates:\n",
        "            if candidate in df.columns:\n",
        "                col_map[target] = candidate\n",
        "                break\n",
        "\n",
        "    if len(col_map) >= 4:\n",
        "        shipments[\"origin_lat\"] = pd.to_numeric(df[col_map[\"pickup_latitude\"]], errors='coerce')\n",
        "        shipments[\"origin_lon\"] = pd.to_numeric(df[col_map[\"pickup_longitude\"]], errors='coerce')\n",
        "        shipments[\"dest_lat\"] = pd.to_numeric(df[col_map[\"dropoff_latitude\"]], errors='coerce')\n",
        "        shipments[\"dest_lon\"] = pd.to_numeric(df[col_map[\"dropoff_longitude\"]], errors='coerce')\n",
        "    else:\n",
        "        print(\"Could not find lat/lon columns, using Chicago area defaults\")\n",
        "        # Use Chicago coordinates with random noise\n",
        "        np.random.seed(42)\n",
        "        n = len(df)\n",
        "        shipments[\"origin_lat\"] = 41.8781 + np.random.normal(0, 0.05, n)\n",
        "        shipments[\"origin_lon\"] = -87.6298 + np.random.normal(0, 0.05, n)\n",
        "        shipments[\"dest_lat\"] = 41.8781 + np.random.normal(0, 0.05, n)\n",
        "        shipments[\"dest_lon\"] = -87.6298 + np.random.normal(0, 0.05, n)\n",
        "\n",
        "    # Calculate distances\n",
        "    print(\"Computing distances...\")\n",
        "    shipments[\"distance_km\"] = shipments.apply(\n",
        "        lambda r: haversine_km((r[\"origin_lat\"], r[\"origin_lon\"]), (r[\"dest_lat\"], r[\"dest_lon\"])),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Estimate weights\n",
        "    shipments[\"weight_kg\"] = 75.0  # Average passenger + luggage weight\n",
        "    shipments[\"tonnes\"] = shipments[\"weight_kg\"] / 1000.0\n",
        "    shipments[\"transport_mode\"] = \"taxi\"\n",
        "    shipments[\"package_type\"] = \"passenger_transport\"\n",
        "\n",
        "    # Clean data\n",
        "    shipments = shipments.replace([np.inf, -np.inf], np.nan)\n",
        "    shipments = shipments.dropna(subset=[\"origin_lat\", \"dest_lat\", \"distance_km\"])\n",
        "    shipments = shipments[shipments[\"distance_km\"] > 0.1]\n",
        "    shipments = shipments[shipments[\"distance_km\"] < 200]\n",
        "    shipments = shipments.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Processed {len(shipments)} valid shipments\")\n",
        "    shipments.to_parquet(out_path)\n",
        "    print(f\"Saved processed data to {out_path}\")\n",
        "\n",
        "    return out_path\n",
        "\n",
        "# -----------------------------\n",
        "# Emissions calculations (FIXED)\n",
        "# -----------------------------\n",
        "def calc_emissions_tkm_method(shipments: pd.DataFrame, mode_col: str = \"transport_mode\", tkm_factors: dict = None):\n",
        "    \"\"\"Calculate emissions using tonne-km method - FIXED VERSION\"\"\"\n",
        "    tkm_factors = tkm_factors or DEFAULT_TKM_FACTORS\n",
        "\n",
        "    print(\"Calculating emissions using tonne-km method...\")\n",
        "    print(f\"Available emission factors: {tkm_factors}\")\n",
        "\n",
        "    def calculate_emission_per_row(row):\n",
        "        mode = \"truck\"  # default fallback\n",
        "        if mode_col in row.index and pd.notna(row[mode_col]):\n",
        "            mode = str(row[mode_col]).lower().strip()\n",
        "\n",
        "        # Get emission factor\n",
        "        factor = tkm_factors.get(mode, tkm_factors.get(\"truck\", 120.0))\n",
        "\n",
        "        # Calculate: distance_km * tonnes * emission_factor_g_per_tonne_km\n",
        "        tonne_km = row[\"distance_km\"] * row[\"tonnes\"]\n",
        "        emission_grams = tonne_km * factor\n",
        "        emission_tonnes = emission_grams / 1e6  # Convert grams to tonnes\n",
        "\n",
        "        return emission_tonnes\n",
        "\n",
        "    shipments[\"emission_tonnes_tkm\"] = shipments.apply(calculate_emission_per_row, axis=1)\n",
        "\n",
        "    # Debug information\n",
        "    total_emissions = shipments[\"emission_tonnes_tkm\"].sum()\n",
        "    print(f\"Total emissions calculated (TKM method): {total_emissions:.6f} tonnes\")\n",
        "    print(f\"Average emission per shipment: {total_emissions/len(shipments)*1000:.3f} kg\")\n",
        "\n",
        "    return shipments\n",
        "\n",
        "def calc_emissions_fuel_estimate(shipments: pd.DataFrame, mode_col: str = \"transport_mode\"):\n",
        "    \"\"\"Calculate emissions using fuel consumption estimates - FIXED VERSION\"\"\"\n",
        "    print(\"Calculating emissions using fuel consumption method...\")\n",
        "\n",
        "    def calculate_fuel_emission_per_row(row):\n",
        "        mode = str(row.get(mode_col, \"truck\")).lower().strip()\n",
        "\n",
        "        # Get consumption rate (liters per 100km)\n",
        "        consumption_per_100km = DEFAULT_CONSUMPTION.get(mode, DEFAULT_CONSUMPTION.get(\"taxi\", 8.5))\n",
        "\n",
        "        # Choose fuel type based on mode\n",
        "        if mode in [\"truck\", \"rail\", \"ship\"]:\n",
        "            fuel_type = \"diesel\"\n",
        "        else:\n",
        "            fuel_type = \"gasoline\"\n",
        "\n",
        "        fuel_factor = DEFAULT_FUEL_FACTORS[fuel_type]  # grams CO2 per liter\n",
        "\n",
        "        # Calculate fuel consumption for this trip\n",
        "        fuel_liters = (consumption_per_100km * row[\"distance_km\"]) / 100.0\n",
        "\n",
        "        # Calculate emissions\n",
        "        emission_grams = fuel_liters * fuel_factor\n",
        "        emission_tonnes = emission_grams / 1e6  # Convert to tonnes\n",
        "\n",
        "        return emission_tonnes\n",
        "\n",
        "    shipments[\"emission_tonnes_fuel\"] = shipments.apply(calculate_fuel_emission_per_row, axis=1)\n",
        "\n",
        "    # Debug information\n",
        "    total_emissions = shipments[\"emission_tonnes_fuel\"].sum()\n",
        "    print(f\"Total emissions calculated (Fuel method): {total_emissions:.6f} tonnes\")\n",
        "    print(f\"Average emission per shipment: {total_emissions/len(shipments)*1000:.3f} kg\")\n",
        "\n",
        "    return shipments\n",
        "\n",
        "def choose_final_emission(shipments: pd.DataFrame):\n",
        "    \"\"\"Choose final emission values with proper fallback logic\"\"\"\n",
        "    # Use TKM method as primary, fuel estimate as backup\n",
        "    shipments[\"emission_tonnes_final\"] = shipments[\"emission_tonnes_tkm\"].fillna(\n",
        "        shipments[\"emission_tonnes_fuel\"]\n",
        "    ).fillna(0.0)\n",
        "\n",
        "    # Ensure no negative emissions\n",
        "    shipments[\"emission_tonnes_final\"] = shipments[\"emission_tonnes_final\"].clip(lower=0.0)\n",
        "\n",
        "    # Convert to kg for easier reading\n",
        "    shipments[\"emission_kg_final\"] = shipments[\"emission_tonnes_final\"] * 1000\n",
        "\n",
        "    print(f\"Final emissions summary:\")\n",
        "    print(f\"  Total: {shipments['emission_tonnes_final'].sum():.6f} tonnes\")\n",
        "    print(f\"  Average per shipment: {shipments['emission_kg_final'].mean():.3f} kg\")\n",
        "    print(f\"  Max per shipment: {shipments['emission_kg_final'].max():.3f} kg\")\n",
        "\n",
        "    return shipments\n",
        "\n",
        "# -----------------------------\n",
        "# VRP optimization functions\n",
        "# -----------------------------\n",
        "def create_distance_matrix(locations):\n",
        "    \"\"\"Create distance matrix for VRP solver\"\"\"\n",
        "    n = len(locations)\n",
        "    mat = [[0]*n for _ in range(n)]\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i != j:\n",
        "                mat[i][j] = int(haversine_km(locations[i], locations[j]) * 1000)  # Convert to meters\n",
        "    return mat\n",
        "\n",
        "def simple_route_optimization(shipments: pd.DataFrame, num_vehicles: int = 3, depot_coord=None):\n",
        "    \"\"\"Improved route optimization using nearest neighbor and 2-opt\"\"\"\n",
        "    if len(shipments) == 0:\n",
        "        return []\n",
        "\n",
        "    print(\"Using nearest neighbor + 2-opt optimization\")\n",
        "\n",
        "    # Use destination coordinates\n",
        "    depot_coord = depot_coord or (shipments[\"dest_lat\"].mean(), shipments[\"dest_lon\"].mean())\n",
        "    locations = [depot_coord] + list(zip(shipments[\"dest_lat\"].values, shipments[\"dest_lon\"].values))\n",
        "    demands = [0] + shipments[\"weight_kg\"].tolist()\n",
        "\n",
        "    # Create distance matrix\n",
        "    n = len(locations)\n",
        "    distance_matrix = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i != j:\n",
        "                distance_matrix[i][j] = haversine_km(locations[i], locations[j])\n",
        "\n",
        "    routes = []\n",
        "    unassigned = list(range(1, n))  # All locations except depot (0)\n",
        "    vehicle_capacity = max(1000, max(demands) * 1.5)  # Adaptive capacity\n",
        "\n",
        "    for vehicle_id in range(num_vehicles):\n",
        "        if not unassigned:\n",
        "            break\n",
        "\n",
        "        route = [0]  # Start at depot\n",
        "        current_load = 0\n",
        "        current_location = 0\n",
        "        route_distance = 0\n",
        "\n",
        "        while unassigned:\n",
        "            # Find nearest unassigned location that fits capacity\n",
        "            best_location = None\n",
        "            best_distance = float('inf')\n",
        "\n",
        "            for loc in unassigned:\n",
        "                if current_load + demands[loc] <= vehicle_capacity:\n",
        "                    distance = distance_matrix[current_location][loc]\n",
        "                    if distance < best_distance:\n",
        "                        best_distance = distance\n",
        "                        best_location = loc\n",
        "\n",
        "            if best_location is None:\n",
        "                break  # No more locations fit in this vehicle\n",
        "\n",
        "            # Add location to route\n",
        "            route.append(best_location)\n",
        "            unassigned.remove(best_location)\n",
        "            current_load += demands[best_location]\n",
        "            route_distance += best_distance\n",
        "            current_location = best_location\n",
        "\n",
        "        # Return to depot\n",
        "        if len(route) > 1:\n",
        "            route_distance += distance_matrix[current_location][0]\n",
        "\n",
        "            # Apply 2-opt improvement\n",
        "            route, route_distance = improve_route_2opt(route, distance_matrix)\n",
        "\n",
        "            routes.append({\n",
        "                \"vehicle_id\": f\"V{vehicle_id+1}\",\n",
        "                \"route_nodes\": route,\n",
        "                \"route_distance_km\": route_distance,\n",
        "                \"route_load_kg\": sum(demands[i] for i in route[1:]),  # Exclude depot\n",
        "                \"stops\": len(route) - 1  # Excluding depot\n",
        "            })\n",
        "\n",
        "    return routes\n",
        "\n",
        "def improve_route_2opt(route, distance_matrix, max_iterations=50):\n",
        "    \"\"\"Improve route using 2-opt local search\"\"\"\n",
        "    if len(route) <= 3:\n",
        "        return route, calculate_route_distance(route, distance_matrix)\n",
        "\n",
        "    best_route = route[:]\n",
        "    best_distance = calculate_route_distance(route, distance_matrix)\n",
        "\n",
        "    improved = True\n",
        "    iteration = 0\n",
        "\n",
        "    while improved and iteration < max_iterations:\n",
        "        improved = False\n",
        "        iteration += 1\n",
        "\n",
        "        for i in range(1, len(route) - 2):\n",
        "            for j in range(i + 1, len(route)):\n",
        "                if j - i == 1:\n",
        "                    continue  # Skip adjacent edges\n",
        "\n",
        "                # Create new route by reversing segment between i and j\n",
        "                new_route = route[:i] + route[i:j][::-1] + route[j:]\n",
        "                new_distance = calculate_route_distance(new_route, distance_matrix)\n",
        "\n",
        "                if new_distance < best_distance:\n",
        "                    best_route = new_route[:]\n",
        "                    best_distance = new_distance\n",
        "                    route = new_route[:]\n",
        "                    improved = True\n",
        "                    break\n",
        "\n",
        "            if improved:\n",
        "                break\n",
        "\n",
        "    return best_route, best_distance\n",
        "\n",
        "def calculate_route_distance(route, distance_matrix):\n",
        "    \"\"\"Calculate total distance for a route\"\"\"\n",
        "    total_distance = 0\n",
        "    for i in range(len(route) - 1):\n",
        "        total_distance += distance_matrix[route[i]][route[i + 1]]\n",
        "    return total_distance\n",
        "\n",
        "def clustering_based_optimization(shipments: pd.DataFrame, num_vehicles: int = 3, depot_coord=None):\n",
        "    \"\"\"Cluster-based route optimization using K-means\"\"\"\n",
        "    if not SKLEARN_AVAILABLE:\n",
        "        print(\"Scikit-learn not available. Using simple nearest neighbor.\")\n",
        "        return simple_route_optimization(shipments, num_vehicles, depot_coord)\n",
        "\n",
        "    print(\"Using K-means clustering + route optimization\")\n",
        "\n",
        "    # Get coordinates\n",
        "    coords = shipments[[\"dest_lat\", \"dest_lon\"]].values\n",
        "    depot_coord = depot_coord or (coords[:, 0].mean(), coords[:, 1].mean())\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=min(num_vehicles, len(shipments)), random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(coords)\n",
        "\n",
        "    routes = []\n",
        "\n",
        "    for vehicle_id in range(num_vehicles):\n",
        "        cluster_mask = clusters == vehicle_id\n",
        "        if not cluster_mask.any():\n",
        "            continue\n",
        "\n",
        "        cluster_shipments = shipments[cluster_mask].copy()\n",
        "\n",
        "        if len(cluster_shipments) == 0:\n",
        "            continue\n",
        "\n",
        "        # Create route for this cluster using nearest neighbor\n",
        "        locations = [depot_coord] + list(zip(cluster_shipments[\"dest_lat\"], cluster_shipments[\"dest_lon\"]))\n",
        "        route_indices = nearest_neighbor_route(locations)\n",
        "\n",
        "        # Calculate route statistics\n",
        "        route_distance = 0\n",
        "        for i in range(len(route_indices) - 1):\n",
        "            route_distance += haversine_km(locations[route_indices[i]], locations[route_indices[i + 1]])\n",
        "\n",
        "        # Return to depot\n",
        "        route_distance += haversine_km(locations[route_indices[-1]], locations[0])\n",
        "\n",
        "        route_load = cluster_shipments[\"weight_kg\"].sum()\n",
        "\n",
        "        routes.append({\n",
        "            \"vehicle_id\": f\"V{vehicle_id+1}\",\n",
        "            \"route_nodes\": route_indices,\n",
        "            \"route_distance_km\": route_distance,\n",
        "            \"route_load_kg\": route_load,\n",
        "            \"stops\": len(cluster_shipments)\n",
        "        })\n",
        "\n",
        "    return routes\n",
        "\n",
        "def nearest_neighbor_route(locations):\n",
        "    \"\"\"Create route using nearest neighbor heuristic\"\"\"\n",
        "    n = len(locations)\n",
        "    if n <= 2:\n",
        "        return list(range(n))\n",
        "\n",
        "    unvisited = set(range(1, n))  # Exclude depot (0)\n",
        "    route = [0]  # Start at depot\n",
        "    current = 0\n",
        "\n",
        "    while unvisited:\n",
        "        nearest = min(unvisited, key=lambda x: haversine_km(locations[current], locations[x]))\n",
        "        route.append(nearest)\n",
        "        unvisited.remove(nearest)\n",
        "        current = nearest\n",
        "\n",
        "    return route\n",
        "\n",
        "def vrp_minimize_co2(shipments: pd.DataFrame, num_vehicles: int = 3, vehicle_capacity_kg: int = 2000, depot_coord=None):\n",
        "    \"\"\"Solve VRP to minimize CO2 emissions - tries ortools first, then fallback methods\"\"\"\n",
        "    if len(shipments) == 0:\n",
        "        return []\n",
        "\n",
        "    # Try ortools first\n",
        "    if ensure_ortools_import():\n",
        "        return vrp_with_ortools(shipments, num_vehicles, vehicle_capacity_kg, depot_coord)\n",
        "\n",
        "    # Fallback to alternative methods\n",
        "    try:\n",
        "        # Try clustering-based approach first\n",
        "        if SKLEARN_AVAILABLE:\n",
        "            return clustering_based_optimization(shipments, num_vehicles, depot_coord)\n",
        "        else:\n",
        "            return simple_route_optimization(shipments, num_vehicles, depot_coord)\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization error: {e}\")\n",
        "        return simple_route_optimization(shipments, num_vehicles, depot_coord)\n",
        "\n",
        "def vrp_with_ortools(shipments: pd.DataFrame, num_vehicles: int = 3, vehicle_capacity_kg: int = 2000, depot_coord=None):\n",
        "    \"\"\"Original VRP implementation using ortools\"\"\"\n",
        "    # Limit to reasonable size for demo\n",
        "    if len(shipments) > 20:\n",
        "        shipments = shipments.sample(20, random_state=42)\n",
        "        print(f\"Limited VRP to 20 locations for performance\")\n",
        "\n",
        "    depot_coord = depot_coord or (shipments[\"dest_lat\"].mean(), shipments[\"dest_lon\"].mean())\n",
        "    locations = [depot_coord] + list(zip(shipments[\"dest_lat\"].values, shipments[\"dest_lon\"].values))\n",
        "    demands = [0] + shipments[\"weight_kg\"].astype(int).tolist()\n",
        "    vehicle_capacities = [vehicle_capacity_kg] * num_vehicles\n",
        "\n",
        "    # Create distance matrix (in meters for ortools)\n",
        "    distance_matrix = []\n",
        "    for i, loc1 in enumerate(locations):\n",
        "        row = []\n",
        "        for j, loc2 in enumerate(locations):\n",
        "            if i == j:\n",
        "                row.append(0)\n",
        "            else:\n",
        "                distance_m = int(haversine_km(loc1, loc2) * 1000)\n",
        "                row.append(distance_m)\n",
        "        distance_matrix.append(row)\n",
        "\n",
        "    manager = pywrapcp.RoutingIndexManager(len(distance_matrix), num_vehicles, 0)\n",
        "    routing = pywrapcp.RoutingModel(manager)\n",
        "\n",
        "    def distance_callback(from_index, to_index):\n",
        "        return distance_matrix[manager.IndexToNode(from_index)][manager.IndexToNode(to_index)]\n",
        "\n",
        "    transit_cb_idx = routing.RegisterTransitCallback(distance_callback)\n",
        "    routing.SetArcCostEvaluatorOfAllVehicles(transit_cb_idx)\n",
        "\n",
        "    def demand_callback(from_index):\n",
        "        return demands[manager.IndexToNode(from_index)]\n",
        "\n",
        "    demand_cb_idx = routing.RegisterUnaryTransitCallback(demand_callback)\n",
        "    routing.AddDimensionWithVehicleCapacity(demand_cb_idx, 0, vehicle_capacities, True, \"Capacity\")\n",
        "\n",
        "    # Add penalty for unvisited nodes\n",
        "    penalty = 10**6\n",
        "    for node in range(1, len(locations)):\n",
        "        routing.AddDisjunction([manager.NodeToIndex(node)], penalty)\n",
        "\n",
        "    search_params = pywrapcp.DefaultRoutingSearchParameters()\n",
        "    search_params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC\n",
        "    search_params.time_limit.seconds = 30\n",
        "\n",
        "    solution = routing.SolveWithParameters(search_params)\n",
        "    routes = []\n",
        "\n",
        "    if solution:\n",
        "        for v in range(num_vehicles):\n",
        "            index = routing.Start(v)\n",
        "            route_nodes = []\n",
        "            route_distance_m = 0\n",
        "            route_load = 0\n",
        "\n",
        "            while not routing.IsEnd(index):\n",
        "                node_idx = manager.IndexToNode(index)\n",
        "                route_nodes.append(node_idx)\n",
        "                if node_idx > 0:  # Skip depot\n",
        "                    route_load += demands[node_idx]\n",
        "                previous = index\n",
        "                index = solution.Value(routing.NextVar(index))\n",
        "                route_distance_m += routing.GetArcCostForVehicle(previous, index, v)\n",
        "\n",
        "            if len(route_nodes) > 1:  # Only include routes with actual stops\n",
        "                routes.append({\n",
        "                    \"vehicle_id\": f\"V{v+1}\",\n",
        "                    \"route_nodes\": route_nodes,\n",
        "                    \"route_distance_km\": route_distance_m / 1000.0,\n",
        "                    \"route_load_kg\": route_load,\n",
        "                    \"stops\": len(route_nodes) - 1  # Excluding depot\n",
        "                })\n",
        "\n",
        "    return routes\n",
        "\n",
        "# -----------------------------\n",
        "# Reporting (Enhanced)\n",
        "# -----------------------------\n",
        "def generate_summary_report(shipments: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Generate comprehensive summary report\"\"\"\n",
        "    total_shipments = len(shipments)\n",
        "    total_weight_tonnes = shipments[\"tonnes\"].sum()\n",
        "    total_distance_km = shipments[\"distance_km\"].sum()\n",
        "    total_tkm = (shipments[\"distance_km\"] * shipments[\"tonnes\"]).sum()\n",
        "    total_co2_tonnes = shipments[\"emission_tonnes_final\"].sum()\n",
        "    avg_co2_per_shipment = total_co2_tonnes / total_shipments if total_shipments > 0 else 0\n",
        "\n",
        "    # Mode breakdown\n",
        "    mode_breakdown = {}\n",
        "    if \"transport_mode\" in shipments.columns:\n",
        "        mode_stats = shipments.groupby(\"transport_mode\").agg({\n",
        "            \"shipment_id\": \"count\",\n",
        "            \"emission_tonnes_final\": \"sum\",\n",
        "            \"distance_km\": \"sum\",\n",
        "            \"weight_kg\": \"sum\"\n",
        "        }).round(4)\n",
        "\n",
        "        mode_breakdown = {\n",
        "            mode: {\n",
        "                \"shipment_count\": int(row[\"shipment_id\"]),\n",
        "                \"total_emissions_tonnes\": float(row[\"emission_tonnes_final\"]),\n",
        "                \"total_distance_km\": float(row[\"distance_km\"]),\n",
        "                \"total_weight_kg\": float(row[\"weight_kg\"]),\n",
        "                \"avg_emissions_per_shipment_kg\": float(row[\"emission_tonnes_final\"] * 1000 / row[\"shipment_id\"]) if row[\"shipment_id\"] > 0 else 0\n",
        "            }\n",
        "            for mode, row in mode_stats.iterrows()\n",
        "        }\n",
        "\n",
        "    report = {\n",
        "        \"total_shipments\": total_shipments,\n",
        "        \"total_weight_tonnes\": round(total_weight_tonnes, 4),\n",
        "        \"total_distance_km\": round(total_distance_km, 2),\n",
        "        \"total_tonne_km\": round(total_tkm, 2),\n",
        "        \"total_co2_tonnes\": round(total_co2_tonnes, 6),\n",
        "        \"total_co2_kg\": round(total_co2_tonnes * 1000, 3),\n",
        "        \"avg_co2_per_shipment_kg\": round(avg_co2_per_shipment * 1000, 3),\n",
        "        \"co2_intensity_kg_per_tkm\": round((total_co2_tonnes * 1000) / total_tkm, 4) if total_tkm > 0 else 0,\n",
        "        \"avg_distance_per_shipment_km\": round(total_distance_km / total_shipments, 2) if total_shipments > 0 else 0,\n",
        "        \"avg_weight_per_shipment_kg\": round(total_weight_tonnes * 1000 / total_shipments, 2) if total_shipments > 0 else 0,\n",
        "        \"mode_breakdown\": mode_breakdown\n",
        "    }\n",
        "\n",
        "    return report\n",
        "\n",
        "def save_outputs(shipments: pd.DataFrame, out_dir: Path):\n",
        "    \"\"\"Save analysis outputs and print detailed data\"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save detailed data (parquet only)\n",
        "    shipments.to_parquet(out_dir / \"shipments_emissions.parquet\")\n",
        "\n",
        "    # Generate and save summary report\n",
        "    report = generate_summary_report(shipments)\n",
        "\n",
        "    import json\n",
        "    with open(out_dir / \"summary_report.json\", \"w\") as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(f\"Saved processed shipments to {out_dir}\")\n",
        "\n",
        "    # Print detailed shipments data directly to console\n",
        "    print(\"\\n=== DETAILED SHIPMENTS DATA ===\")\n",
        "    print(f\"Showing first 10 shipments out of {len(shipments)} total:\")\n",
        "\n",
        "    # Select key columns for display\n",
        "    display_cols = [\n",
        "        'shipment_id', 'transport_mode', 'distance_km', 'weight_kg',\n",
        "        'emission_kg_final', 'passenger_count'\n",
        "    ]\n",
        "\n",
        "    # Only show columns that exist\n",
        "    available_cols = [col for col in display_cols if col in shipments.columns]\n",
        "    display_data = shipments[available_cols].head(10).copy()\n",
        "\n",
        "    # Format for better readability\n",
        "    if 'distance_km' in display_data.columns:\n",
        "        display_data['distance_km'] = display_data['distance_km'].round(2)\n",
        "    if 'weight_kg' in display_data.columns:\n",
        "        display_data['weight_kg'] = display_data['weight_kg'].round(1)\n",
        "    if 'emission_kg_final' in display_data.columns:\n",
        "        display_data['emission_kg_final'] = display_data['emission_kg_final'].round(3)\n",
        "\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.max_colwidth', 15)\n",
        "\n",
        "    print(display_data.to_string(index=False))\n",
        "\n",
        "    if len(shipments) > 10:\n",
        "        print(f\"\\n... and {len(shipments) - 10} more shipments\")\n",
        "\n",
        "    return report\n",
        "\n",
        "# -----------------------------\n",
        "# Full pipeline (Enhanced)\n",
        "# -----------------------------\n",
        "def run_full_pipeline(dataset: str = \"nyc_taxi\", sample_n: int = 10000):\n",
        "    \"\"\"Run the complete carbon footprint analysis pipeline\"\"\"\n",
        "\n",
        "    print(f\"=== Carbon Footprint Analysis Pipeline ===\")\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    print(f\"Sample size: {sample_n}\")\n",
        "\n",
        "    # Step 1: Download and process data based on dataset choice\n",
        "    if dataset == \"nyc_taxi\":\n",
        "        raw_data_path = DATA_DIR / DATASET_CONFIGS[\"nyc_taxi\"][\"file_name\"]\n",
        "        processed_data_path = PROCESSED_DIR / \"nyc_taxi_shipments.parquet\"\n",
        "\n",
        "        if not raw_data_path.exists():\n",
        "            download_nyc_taxi_data(raw_data_path)\n",
        "\n",
        "        if not processed_data_path.exists():\n",
        "            process_nyc_taxi_data(raw_data_path, processed_data_path, sample_n)\n",
        "\n",
        "    elif dataset == \"chicago_taxi\":\n",
        "        raw_data_path = DATA_DIR / DATASET_CONFIGS[\"chicago_taxi\"][\"file_name\"]\n",
        "        processed_data_path = PROCESSED_DIR / \"chicago_taxi_shipments.parquet\"\n",
        "\n",
        "        if not raw_data_path.exists():\n",
        "            download_chicago_taxi_data(raw_data_path)\n",
        "\n",
        "        if not processed_data_path.exists():\n",
        "            process_chicago_taxi_data(raw_data_path, processed_data_path, sample_n)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset}. Choose from: nyc_taxi, chicago_taxi\")\n",
        "\n",
        "    # Step 2: Load processed shipments data\n",
        "    try:\n",
        "        shipments = pd.read_parquet(processed_data_path)\n",
        "        print(f\"Loaded {len(shipments)} shipments for analysis\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading processed data: {e}\")\n",
        "        print(\"Reprocessing data...\")\n",
        "        if dataset == \"nyc_taxi\":\n",
        "            process_nyc_taxi_data(raw_data_path, processed_data_path, sample_n)\n",
        "        elif dataset == \"chicago_taxi\":\n",
        "            process_chicago_taxi_data(raw_data_path, processed_data_path, sample_n)\n",
        "        shipments = pd.read_parquet(processed_data_path)\n",
        "\n",
        "    if len(shipments) == 0:\n",
        "        raise ValueError(\"No shipments loaded. Check data processing.\")\n",
        "\n",
        "    # Step 3: Calculate emissions\n",
        "    print(\"\\n=== CALCULATING EMISSIONS ===\")\n",
        "    shipments = calc_emissions_tkm_method(shipments)\n",
        "    shipments = calc_emissions_fuel_estimate(shipments)\n",
        "    shipments = choose_final_emission(shipments)\n",
        "\n",
        "    # Step 4: Generate report\n",
        "    report = save_outputs(shipments, PROCESSED_DIR)\n",
        "\n",
        "    print(\"\\n=== SUMMARY REPORT ===\")\n",
        "    print(f\"Total shipments: {report['total_shipments']:,}\")\n",
        "    print(f\"Total weight: {report['total_weight_tonnes']:,.4f} tonnes\")\n",
        "    print(f\"Total distance: {report['total_distance_km']:,.1f} km\")\n",
        "    print(f\"Total tonne-km: {report['total_tonne_km']:,.1f}\")\n",
        "    print(f\"Total CO2 emissions: {report['total_co2_kg']:,.1f} kg ({report['total_co2_tonnes']:.6f} tonnes)\")\n",
        "    print(f\"Average CO2 per shipment: {report['avg_co2_per_shipment_kg']:.3f} kg\")\n",
        "    print(f\"Average distance per shipment: {report['avg_distance_per_shipment_km']:.1f} km\")\n",
        "    print(f\"CO2 intensity: {report['co2_intensity_kg_per_tkm']:.4f} kg CO2/tonne-km\")\n",
        "\n",
        "    if report['mode_breakdown']:\n",
        "        print(\"\\n=== MODE BREAKDOWN ===\")\n",
        "        for mode, data in report['mode_breakdown'].items():\n",
        "            print(f\"{mode.upper()}:\")\n",
        "            print(f\"  Shipments: {data['shipment_count']:,}\")\n",
        "            print(f\"  Total emissions: {data['total_emissions_tonnes']*1000:.1f} kg\")\n",
        "            print(f\"  Avg per shipment: {data['avg_emissions_per_shipment_kg']:.3f} kg\")\n",
        "            print(f\"  Total distance: {data['total_distance_km']:.1f} km\")\n",
        "\n",
        "    # Step 5: VRP optimization (sample)\n",
        "    sample_size = min(50, len(shipments))  # Limit sample size for performance\n",
        "    sample_shipments = shipments.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n=== VRP OPTIMIZATION (sample of {sample_size} shipments) ===\")\n",
        "\n",
        "    # Check if user wants to install ortools\n",
        "    if not pywrapcp:\n",
        "        print(\"Note: For advanced VRP optimization, install ortools:\")\n",
        "        print(\"  pip install ortools\")\n",
        "        print(\"Using alternative optimization methods...\")\n",
        "\n",
        "    routes = vrp_minimize_co2(sample_shipments, num_vehicles=3)\n",
        "\n",
        "    if routes:\n",
        "        print(\"Optimized routes:\")\n",
        "        total_route_distance = 0\n",
        "        total_route_load = 0\n",
        "\n",
        "        for i, route in enumerate(routes):\n",
        "            print(f\"  Route {i+1}: {route['stops']} stops, {route['route_distance_km']:.1f} km, {route['route_load_kg']:.0f} kg load\")\n",
        "            total_route_distance += route['route_distance_km']\n",
        "            total_route_load += route['route_load_kg']\n",
        "\n",
        "        print(f\"Total optimized distance: {total_route_distance:.1f} km\")\n",
        "        print(f\"Total load transported: {total_route_load:.0f} kg\")\n",
        "\n",
        "        # Calculate potential savings\n",
        "        original_distance = sample_shipments[\"distance_km\"].sum()\n",
        "        distance_savings = max(0, original_distance - total_route_distance)\n",
        "        savings_percent = (distance_savings/original_distance*100) if original_distance > 0 else 0\n",
        "        print(f\"Potential distance savings: {distance_savings:.1f} km ({savings_percent:.1f}%)\")\n",
        "\n",
        "        # Calculate CO2 savings\n",
        "        original_co2 = sample_shipments[\"emission_kg_final\"].sum()\n",
        "        optimized_co2 = original_co2 * (total_route_distance / original_distance) if original_distance > 0 else 0\n",
        "        co2_savings = max(0, original_co2 - optimized_co2)\n",
        "        print(f\"Potential CO2 savings: {co2_savings:.3f} kg ({co2_savings/original_co2*100:.1f}%)\")\n",
        "\n",
        "        # Save routes\n",
        "        import json\n",
        "        with open(PROCESSED_DIR / \"vrp_routes.json\", \"w\") as f:\n",
        "            json.dump(routes, f, indent=2)\n",
        "\n",
        "        # Print detailed routes to console\n",
        "        print(f\"\\n=== DETAILED VRP ROUTES ===\")\n",
        "        for i, route in enumerate(routes):\n",
        "            print(f\"\\nRoute {i+1} (Vehicle {route['vehicle_id']}):\")\n",
        "            print(f\"  Stops: {route['stops']}\")\n",
        "            print(f\"  Distance: {route['route_distance_km']:.1f} km\")\n",
        "            print(f\"  Load: {route['route_load_kg']:.0f} kg\")\n",
        "            print(f\"  Route sequence: {' -> '.join(map(str, route['route_nodes']))}\")\n",
        "\n",
        "        print(f\"\\nSaved VRP routes to {PROCESSED_DIR / 'vrp_routes.json'}\")\n",
        "    else:\n",
        "        print(\"No routes generated. This could be due to:\")\n",
        "        print(\"  - Very small dataset\")\n",
        "        print(\"  - All shipments too heavy for vehicle capacity\")\n",
        "        print(\"  - Optimization algorithm issues\")\n",
        "        print(\"\\nTry:\")\n",
        "        print(\"  1. Installing ortools: pip install ortools\")\n",
        "        print(\"  2. Using a larger sample size: --sample_n 50000\")\n",
        "        print(\"  3. Installing scikit-learn for clustering: pip install scikit-learn\")\n",
        "\n",
        "    # Step 6: Additional analysis\n",
        "    print(\"\\n=== ADDITIONAL INSIGHTS ===\")\n",
        "\n",
        "    # Distance distribution\n",
        "    distance_stats = shipments[\"distance_km\"].describe()\n",
        "    print(f\"Distance statistics:\")\n",
        "    print(f\"  Mean: {distance_stats['mean']:.1f} km\")\n",
        "    print(f\"  Median: {distance_stats['50%']:.1f} km\")\n",
        "    print(f\"  Max: {distance_stats['max']:.1f} km\")\n",
        "    print(f\"  Min: {distance_stats['min']:.1f} km\")\n",
        "\n",
        "    # Emission distribution\n",
        "    emission_stats = shipments[\"emission_kg_final\"].describe()\n",
        "    print(f\"Emission statistics:\")\n",
        "    print(f\"  Mean per shipment: {emission_stats['mean']:.3f} kg CO2\")\n",
        "    print(f\"  Median per shipment: {emission_stats['50%']:.3f} kg CO2\")\n",
        "    print(f\"  Max per shipment: {emission_stats['max']:.3f} kg CO2\")\n",
        "    print(f\"  Min per shipment: {emission_stats['min']:.3f} kg CO2\")\n",
        "\n",
        "    # Top emission routes\n",
        "    top_emitters = shipments.nlargest(5, \"emission_kg_final\")[[\"shipment_id\", \"distance_km\", \"emission_kg_final\"]]\n",
        "    print(f\"\\nTop 5 highest emission shipments:\")\n",
        "    for _, row in top_emitters.iterrows():\n",
        "        print(f\"  {row['shipment_id']}: {row['distance_km']:.1f} km, {row['emission_kg_final']:.3f} kg CO2\")\n",
        "\n",
        "    # Print emission breakdown by distance ranges\n",
        "    print(f\"\\n=== EMISSION BREAKDOWN BY DISTANCE ===\")\n",
        "    shipments_copy = shipments.copy()\n",
        "    shipments_copy['distance_range'] = pd.cut(\n",
        "        shipments_copy['distance_km'],\n",
        "        bins=[0, 2, 5, 10, 25, 50, float('inf')],\n",
        "        labels=['0-2km', '2-5km', '5-10km', '10-25km', '25-50km', '50km+']\n",
        "    )\n",
        "\n",
        "    distance_breakdown = shipments_copy.groupby('distance_range', observed=True).agg({\n",
        "        'shipment_id': 'count',\n",
        "        'emission_kg_final': 'sum',\n",
        "        'distance_km': 'mean'\n",
        "    }).round(3)\n",
        "\n",
        "    if not distance_breakdown.empty:\n",
        "        print(f\"{'Range':<10} {'Count':<8} {'Avg Dist':<10} {'Total CO2 (kg)':<15}\")\n",
        "        print(\"-\" * 50)\n",
        "        for range_name, row in distance_breakdown.iterrows():\n",
        "            if pd.notna(range_name):\n",
        "                print(f\"{range_name:<10} {int(row['shipment_id']):<8} {row['distance_km']:<10.1f} {row['emission_kg_final']:<15.3f}\")\n",
        "\n",
        "    print(f\"\\n=== Pipeline complete! Results saved to {PROCESSED_DIR} ===\")\n",
        "    print(f\"Files generated:\")\n",
        "    print(f\"  - {PROCESSED_DIR}/shipments_emissions.parquet (detailed data)\")\n",
        "    print(f\"  - {PROCESSED_DIR}/summary_report.json (summary statistics)\")\n",
        "    if routes:\n",
        "        print(f\"  - {PROCESSED_DIR}/vrp_routes.json (optimization results)\")\n",
        "\n",
        "    return shipments, report\n",
        "\n",
        "def list_available_datasets():\n",
        "    \"\"\"List all available public datasets\"\"\"\n",
        "    print(\"Available datasets:\")\n",
        "    for key, config in DATASET_CONFIGS.items():\n",
        "        print(f\"  {key}: {config['description']}\")\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Run carbon logistics pipeline with public datasets\")\n",
        "    parser.add_argument(\"--dataset\", choices=[\"nyc_taxi\", \"chicago_taxi\"], default=\"nyc_taxi\",\n",
        "                       help=\"Public dataset to use\")\n",
        "    parser.add_argument(\"--sample_n\", type=int, default=10000,\n",
        "                       help=\"Number of shipments to process\")\n",
        "    parser.add_argument(\"--list_datasets\", action=\"store_true\",\n",
        "                       help=\"List available datasets and exit\")\n",
        "\n",
        "    # Handle Jupyter notebook execution\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    if args.list_datasets:\n",
        "        list_available_datasets()\n",
        "        sys.exit(0)\n",
        "\n",
        "    try:\n",
        "        shipments, report = run_full_pipeline(\n",
        "            dataset=args.dataset,\n",
        "            sample_n=args.sample_n\n",
        "        )\n",
        "        print(\"\\n✅ SUCCESS: Pipeline completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERROR: Pipeline failed: {e}\")\n",
        "        print(\"Try running with --list_datasets to see available options\")\n",
        "        print(\"Or try with a larger sample size: --sample_n 50000\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UK9DavSM-LJ",
        "outputId": "2469d001-e14f-4e83-99e5-3f546f3b5111"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Carbon Footprint Analysis Pipeline ===\n",
            "Dataset: nyc_taxi\n",
            "Sample size: 10000\n",
            "Loaded 10000 shipments for analysis\n",
            "\n",
            "=== CALCULATING EMISSIONS ===\n",
            "Calculating emissions using tonne-km method...\n",
            "Available emission factors: {'truck': 120.0, 'rail': 30.0, 'ship': 15.0, 'air': 600.0, 'taxi': 180.0, 'van': 90.0, 'motorcycle': 80.0}\n",
            "Total emissions calculated (TKM method): 0.957849 tonnes\n",
            "Average emission per shipment: 0.096 kg\n",
            "Calculating emissions using fuel consumption method...\n",
            "Total emissions calculated (Fuel method): 10.546076 tonnes\n",
            "Average emission per shipment: 1.055 kg\n",
            "Final emissions summary:\n",
            "  Total: 0.957849 tonnes\n",
            "  Average per shipment: 0.096 kg\n",
            "  Max per shipment: 3.218 kg\n",
            "Saved processed shipments to processed\n",
            "\n",
            "=== DETAILED SHIPMENTS DATA ===\n",
            "Showing first 10 shipments out of 10000 total:\n",
            "shipment_id transport_mode  distance_km  weight_kg  emission_kg_final  passenger_count\n",
            "     TAXI_0           taxi         1.45       75.0              0.020              1.0\n",
            "     TAXI_1           taxi         1.45       75.0              0.020              1.0\n",
            "     TAXI_2           taxi         1.92       75.0              0.026              1.0\n",
            "     TAXI_3           taxi         2.30      150.0              0.062              2.0\n",
            "     TAXI_4           taxi         3.09       75.0              0.042              1.0\n",
            "     TAXI_5           taxi         1.30       75.0              0.018              1.0\n",
            "     TAXI_6           taxi        30.11       75.0              0.406              1.0\n",
            "     TAXI_7           taxi         2.20       75.0              0.030              1.0\n",
            "     TAXI_8           taxi         5.86       75.0              0.079              1.0\n",
            "     TAXI_9           taxi         2.48       75.0              0.033              1.0\n",
            "\n",
            "... and 9990 more shipments\n",
            "\n",
            "=== SUMMARY REPORT ===\n",
            "Total shipments: 10,000\n",
            "Total weight: 1,002.5250 tonnes\n",
            "Total distance: 51,869.3 km\n",
            "Total tonne-km: 5,321.4\n",
            "Total CO2 emissions: 957.8 kg (0.957849 tonnes)\n",
            "Average CO2 per shipment: 0.096 kg\n",
            "Average distance per shipment: 5.2 km\n",
            "CO2 intensity: 0.1800 kg CO2/tonne-km\n",
            "\n",
            "=== MODE BREAKDOWN ===\n",
            "TAXI:\n",
            "  Shipments: 10,000\n",
            "  Total emissions: 957.8 kg\n",
            "  Avg per shipment: 0.096 kg\n",
            "  Total distance: 51869.4 km\n",
            "\n",
            "=== VRP OPTIMIZATION (sample of 50 shipments) ===\n",
            "Note: For advanced VRP optimization, install ortools:\n",
            "  pip install ortools\n",
            "Using alternative optimization methods...\n",
            "Warning: ortools not installed. Using alternative optimization.\n",
            "Install with: pip install ortools\n",
            "Using K-means clustering + route optimization\n",
            "Optimized routes:\n",
            "  Route 1: 16 stops, 29.0 km, 1500 kg load\n",
            "  Route 2: 30 stops, 47.0 km, 2625 kg load\n",
            "  Route 3: 4 stops, 52.3 km, 525 kg load\n",
            "Total optimized distance: 128.3 km\n",
            "Total load transported: 4650 kg\n",
            "Potential distance savings: 98.9 km (43.5%)\n",
            "Potential CO2 savings: 1.788 kg (43.5%)\n",
            "\n",
            "=== DETAILED VRP ROUTES ===\n",
            "\n",
            "Route 1 (Vehicle V1):\n",
            "  Stops: 16\n",
            "  Distance: 29.0 km\n",
            "  Load: 1500 kg\n",
            "  Route sequence: 0 -> 14 -> 4 -> 8 -> 16 -> 12 -> 1 -> 2 -> 13 -> 15 -> 6 -> 5 -> 7 -> 3 -> 9 -> 10 -> 11\n",
            "\n",
            "Route 2 (Vehicle V2):\n",
            "  Stops: 30\n",
            "  Distance: 47.0 km\n",
            "  Load: 2625 kg\n",
            "  Route sequence: 0 -> 22 -> 4 -> 14 -> 28 -> 18 -> 23 -> 30 -> 2 -> 6 -> 11 -> 3 -> 17 -> 8 -> 10 -> 9 -> 13 -> 15 -> 29 -> 1 -> 7 -> 20 -> 26 -> 5 -> 27 -> 24 -> 25 -> 16 -> 12 -> 21 -> 19\n",
            "\n",
            "Route 3 (Vehicle V3):\n",
            "  Stops: 4\n",
            "  Distance: 52.3 km\n",
            "  Load: 525 kg\n",
            "  Route sequence: 0 -> 4 -> 2 -> 1 -> 3\n",
            "\n",
            "Saved VRP routes to processed/vrp_routes.json\n",
            "\n",
            "=== ADDITIONAL INSIGHTS ===\n",
            "Distance statistics:\n",
            "  Mean: 5.2 km\n",
            "  Median: 2.7 km\n",
            "  Max: 63.1 km\n",
            "  Min: 0.2 km\n",
            "Emission statistics:\n",
            "  Mean per shipment: 0.096 kg CO2\n",
            "  Median per shipment: 0.044 kg CO2\n",
            "  Max per shipment: 3.218 kg CO2\n",
            "  Min per shipment: 0.003 kg CO2\n",
            "\n",
            "Top 5 highest emission shipments:\n",
            "  TAXI_2008: 47.7 km, 3.218 kg CO2\n",
            "  TAXI_8090: 32.3 km, 2.183 kg CO2\n",
            "  TAXI_2965: 24.4 km, 1.975 kg CO2\n",
            "  TAXI_2269: 34.8 km, 1.880 kg CO2\n",
            "  TAXI_7222: 33.9 km, 1.832 kg CO2\n",
            "\n",
            "=== EMISSION BREAKDOWN BY DISTANCE ===\n",
            "Range      Count    Avg Dist   Total CO2 (kg) \n",
            "--------------------------------------------------\n",
            "0-2km      3458     1.3        80.062         \n",
            "2-5km      4026     3.2        232.046        \n",
            "5-10km     1222     6.8        147.512        \n",
            "10-25km    866      15.3       242.582        \n",
            "25-50km    426      30.1       254.013        \n",
            "50km+      2        60.5       1.633          \n",
            "\n",
            "=== Pipeline complete! Results saved to processed ===\n",
            "Files generated:\n",
            "  - processed/shipments_emissions.parquet (detailed data)\n",
            "  - processed/summary_report.json (summary statistics)\n",
            "  - processed/vrp_routes.json (optimization results)\n",
            "\n",
            "✅ SUCCESS: Pipeline completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xq4WyzubN1Ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}